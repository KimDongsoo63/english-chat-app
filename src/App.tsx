import React, { useState, useEffect, useRef } from 'react';
import SpeechRecognition, { useSpeechRecognition } from 'react-speech-recognition';
import OpenAI from 'openai';
import './App.css';

interface Message {
  text: string;
  sender: 'user' | 'assistant' | 'system';
}

interface UserContext {
  proficiencyLevel: 'beginner' | 'intermediate' | 'advanced';
  recentTopics: string[];
  practicedGrammar: string[];
  introducedVocab: string[];
  commonMistakes: string[];
  interests: string[];
  learningGoals: string[];
  preferredTopics: string[];
}

// ë²„ì „ ì •ë³´ì™€ ì›°ì»´ ë©”ì‹œì§€
const VERSION_INFO: Message = {
  text: "",  // ì±„íŒ… ë©”ì‹œì§€ì—ì„œëŠ” ë²„ì „ ì •ë³´ ì œê±°
  sender: 'system'
};

// ì›°ì»´ ë©”ì‹œì§€
const WELCOME_MESSAGE: Message = {
  text: "Hey! How's your day going so far? Found any new books or hobbies lately?",
  sender: 'assistant'
};

// OpenAI client configuration
const openai = new OpenAI({
  apiKey: process.env.REACT_APP_OPENAI_API_KEY,
  dangerouslyAllowBrowser: true
});

// ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê°œì„ 
const SYSTEM_PROMPT = `You are a friendly English conversation partner helping users improve their English speaking skills.

Key Points:
1. Maintain natural conversation flow
2. Adjust your language level to match the user
3. Provide gentle corrections when needed
4. Keep the conversation engaging and encouraging
5. Focus on practical, daily conversation topics

Response Format:
1. First, respond naturally to continue the conversation
2. Then, if needed, add a brief correction (marked with ğŸ’¡)
3. Keep responses concise but natural

Example:
User: "I go to store yesterday"
Assistant: "Oh, you went shopping yesterday! What did you buy? 
ğŸ’¡ Quick tip: Use 'went' for past actions."

Remember:
- Be encouraging and friendly
- Focus on communication over perfect grammar
- Keep the conversation flowing naturally
- Help build confidence`;

function App() {
  const [messages, setMessages] = useState<Message[]>([]);
  const [inputText, setInputText] = useState('');
  const [loading, setLoading] = useState(false);
  const [isListening, setIsListening] = useState(false);
  const [silenceTimer, setSilenceTimer] = useState<NodeJS.Timeout | null>(null);
  const [inactivityTimer, setInactivityTimer] = useState<NodeJS.Timeout | null>(null);
  const [lastUserInteraction, setLastUserInteraction] = useState(Date.now());
  const [deferredPrompt, setDeferredPrompt] = useState<any>(null);
  const [showInstallPrompt, setShowInstallPrompt] = useState(false);
  const currentUtterance = useRef<SpeechSynthesisUtterance | null>(null);
  const messagesEndRef = useRef<HTMLDivElement>(null);
  const [userContext, setUserContext] = useState<UserContext>({
    proficiencyLevel: 'beginner',
    recentTopics: [],
    practicedGrammar: [],
    introducedVocab: [],
    commonMistakes: [],
    interests: [],
    learningGoals: [],
    preferredTopics: []
  });
  const [isUpdateAvailable, setIsUpdateAvailable] = useState(false);
  const [voicesLoaded, setVoicesLoaded] = useState(false);

  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: "smooth" });
  };

  useEffect(() => {
    scrollToBottom();
  }, [messages]);

  const {
    transcript,
    resetTranscript,
    browserSupportsSpeechRecognition,
    isMicrophoneAvailable,
    listening
  } = useSpeechRecognition({
    commands: [
      {
        command: '*',
        callback: () => {
          // ìŒì„± ì¸ì‹ ì¤‘ì—ëŠ” ì…ë ¥ í…ìŠ¤íŠ¸ë§Œ ì—…ë°ì´íŠ¸
          if (!isListening) return;
          setInputText(transcript.trim());
        }
      }
    ]
  });

  // ìŒì„± ì…ë ¥ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë³„ë„ì˜ í•¨ìˆ˜
  const handleVoiceInput = React.useCallback((text: string) => {
    if (!text || loading) return;

    // ì¤‘ë³µ ì²´í¬
    const lastMessage = messages[messages.length - 1];
    if (lastMessage?.text === text && lastMessage?.sender === 'user') {
      console.log('Duplicate voice input detected, ignoring:', text);
      return;
    }

    // ë©”ì‹œì§€ ì¶”ê°€
    const userMessage: Message = {
      text: text,
      sender: 'user'
    };

    setMessages(prev => [...prev, userMessage]);
    handleSendMessage(text, [...messages, userMessage]);
  }, [loading, messages]);

  // ìŒì„± ì¸ì‹ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
  useEffect(() => {
    const handleSpeechStart = () => {
      setIsListening(true);
    };

    const handleSpeechEnd = () => {
      if (isListening && transcript.trim()) {
        handleVoiceInput(transcript.trim());
      } else if (isListening) {
        // ìŒì„±ì´ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ì¢…ë£Œ
        SpeechRecognition.stopListening();
        setIsListening(false);
        resetTranscript();
      }
    };

    if (browserSupportsSpeechRecognition) {
      window.addEventListener('speechend', handleSpeechEnd);
      window.addEventListener('speechstart', handleSpeechStart);
    }

    return () => {
      if (browserSupportsSpeechRecognition) {
        window.removeEventListener('speechend', handleSpeechEnd);
        window.removeEventListener('speechstart', handleSpeechStart);
      }
    };
  }, [browserSupportsSpeechRecognition, isListening, transcript, handleVoiceInput]);

  // Function to handle user inactivity
  const handleInactivity = async () => {
    // ì´ë¯¸ ë¡œë”© ì¤‘ì´ê±°ë‚˜ ìŒì„± ì¸ì‹ ì¤‘ì´ë©´ ë¬´ì‹œ
    if (loading || isListening) return;

    try {
      setLoading(true);
      const response = await openai.chat.completions.create({
        model: "gpt-4-turbo-preview",
        messages: [
          { 
            role: "system", 
            content: `You are helping a beginner practice English conversation.
                     1. Generate a simple question based on daily life.
                     2. Include ONE relevant example to help them understand.
                     3. Keep both question and example very basic and short.
                     4. Use only basic vocabulary and simple grammar.
                     5. Total response should be under 20 words.
                     6. Format: Question + "For example: [simple example]"
                     Example response: "What time do you wake up? For example: I wake up at 7 AM."
                     `
          }
        ],
        temperature: 0.7,
        max_tokens: 50
      });

      const promptText = response.choices[0].message.content || "What did you do today? For example: I went to the park.";
      
      const promptMessage: Message = {
        text: promptText,
        sender: 'assistant'
      };
      
      setMessages(prev => [...prev, promptMessage]);
      speakResponse(promptMessage.text);
    } catch (error) {
      console.error('Error generating prompt:', error);
    } finally {
      setLoading(false);
    }
  };

  // AI ì‘ë‹µ ì²˜ë¦¬ í•¨ìˆ˜ ê°œì„ 
  const handleSendMessage = async (messageText: string, currentMessages: Message[]) => {
    if (loading) {
      console.log('Already processing a message, ignoring:', messageText);
      return;
    }

    setLoading(true);

    try {
      const response = await openai.chat.completions.create({
        model: "gpt-4-turbo-preview",
        messages: [
          { 
            role: "system", 
            content: `You are helping a complete beginner learn English. Follow these rules strictly:
                     1. Use only basic vocabulary and simple grammar
                     2. Keep responses under 15 words
                     3. Use short, simple sentences
                     4. Speak like talking to a friend
                     5. Focus on daily life topics
                     6. If user makes a mistake, correct it very gently
                     7. Never use complex words or idioms
                     8. Never make long explanations
                     9. Never use technical terms
                     10. Always maintain a friendly, encouraging tone`
          },
          ...currentMessages.map(msg => ({
            role: msg.sender === 'user' ? 'user' as const : 'assistant' as const,
            content: msg.text
          }))
        ],
        temperature: 0.7,
        max_tokens: 50
      });

      const aiResponse = response.choices[0].message.content;
      
      if (!aiResponse) {
        throw new Error('No response from AI');
      }

      const assistantMessage: Message = {
        text: aiResponse,
        sender: 'assistant'
      };

      setMessages(prev => [...prev, assistantMessage]);
      
      // ìŒì„± ì¶œë ¥
      setTimeout(() => {
        speakResponse(aiResponse);
        // AI ì‘ë‹µ í›„ 20ì´ˆ íƒ€ì´ë¨¸ ì‹œì‘
        resetInactivityTimer();
      }, 500);
    } catch (error) {
      console.error('Error:', error);
      setMessages(prev => [...prev, {
        text: "I'm sorry, I couldn't understand. Can you say that again?",
        sender: 'assistant'
      }]);
    } finally {
      setLoading(false);
    }
  };

  // ìŒì„± ì¶œë ¥ í•¨ìˆ˜ ê°œì„ 
  const speakResponse = (text: string) => {
    if (!window.speechSynthesis) {
      console.error('Speech synthesis not available');
      return;
    }

    // ì´ì „ ìŒì„± ì¶œë ¥ ì¤‘ì§€
    window.speechSynthesis.cancel();
    if (currentUtterance.current) {
      currentUtterance.current = null;
    }

    // ë¶„ì„ ë¶€ë¶„(ğŸ’¡ ì´í›„) ì œì™¸í•˜ê³  ìŒì„± ì¶œë ¥
    let cleanText = text.split('ğŸ’¡')[0].trim();
    
    const utterance = new SpeechSynthesisUtterance(cleanText);
    
    // ì˜ì–´ ìŒì„± ì„ íƒ
    const voices = window.speechSynthesis.getVoices();
    const englishVoice = voices.find(voice => 
      voice.lang.startsWith('en') && voice.name.includes('Female')
    ) || voices.find(voice => voice.lang.startsWith('en'));
    
    if (englishVoice) {
      utterance.voice = englishVoice;
    }

    // ìŒì„± ì„¤ì •
    utterance.rate = 0.9;  // ìì—°ìŠ¤ëŸ¬ìš´ ì†ë„
    utterance.pitch = 1.0;
    utterance.volume = 1.0;
    utterance.lang = 'en-US';

    // ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
    utterance.onstart = () => {
      currentUtterance.current = utterance;
      console.log('Speech started');
    };

    utterance.onend = () => {
      currentUtterance.current = null;
      console.log('Speech ended');
    };

    utterance.onerror = (event) => {
      console.error('Speech error:', event);
      currentUtterance.current = null;
    };

    // ìŒì„± ì¶œë ¥ ì‹œì‘
    window.speechSynthesis.speak(utterance);
  };

  const handleSend = async () => {
    if (!inputText.trim()) return;
    
    const messageText = inputText.trim();
    
    // ì‚¬ìš©ì ë©”ì‹œì§€ ìƒì„± ë° í‘œì‹œ
    const userMessage: Message = {
      text: messageText,
      sender: 'user'
    };
    
    // ë©”ì‹œì§€ ì¶”ê°€ ë° AI ì‘ë‹µ ìš”ì²­
    setMessages(prev => {
      const newMessages = [...prev, userMessage];
      handleSendMessage(messageText, newMessages);
      return newMessages;
    });
    
    setInputText('');
    resetTranscript();
  };

  // Reset inactivity timer on user interaction
  const resetInactivityTimer = () => {
    if (inactivityTimer) {
      clearTimeout(inactivityTimer);
    }
    const timer = setTimeout(handleInactivity, 20000); // 20ì´ˆ
    setInactivityTimer(timer);
  };

  // ë§ˆì´í¬ ë²„íŠ¼ í´ë¦­ í•¸ë“¤ëŸ¬ ê°œì„ 
  const handleMicClick = async () => {
    // í˜„ì¬ ìŒì„± ì¶œë ¥ ì¤‘ì§€
    if (currentUtterance.current) {
      stopAIVoice();
    }

    // ì´ë¯¸ ì²˜ë¦¬ ì¤‘ì´ë©´ ë¬´ì‹œ
    if (loading) {
      console.log('Loading in progress, ignoring mic click');
      return;
    }

    if (isListening) {
      // STOP ë²„íŠ¼ì„ í´ë¦­í–ˆì„ ë•Œ
      if (transcript.trim()) {
        handleVoiceInput(transcript.trim());
      }
      // ìŒì„± ì¸ì‹ ì¢…ë£Œ
      SpeechRecognition.stopListening();
      setIsListening(false);
      resetTranscript();
    } else {
      try {
        // ë§ˆì´í¬ ì‹œì‘í•  ë•Œ íƒ€ì´ë¨¸ ì´ˆê¸°í™”
        if (inactivityTimer) {
          clearTimeout(inactivityTimer);
          setInactivityTimer(null);
        }
        resetTranscript();
        setInputText('');
        await SpeechRecognition.startListening({
          continuous: true,
          language: 'en-US'
        });
        setIsListening(true);
      } catch (error) {
        console.error('Speech recognition error:', error);
        setIsListening(false);
        setMessages(prev => [...prev, {
          text: "Sorry, there was a problem with the microphone. Please try again.",
          sender: 'system'
        }]);
      }
    }
  };

  // ìŒì„± ì¸ì‹ ìƒíƒœ ë™ê¸°í™”
  useEffect(() => {
    if (!listening && isListening) {
      console.log('Speech recognition stopped unexpectedly');
      if (transcript.trim()) {
        handleVoiceInput(transcript.trim());
      } else {
        setIsListening(false);
        resetTranscript();
      }
    }
  }, [listening]);

  // transcript ë³€ê²½ ê°ì§€
  useEffect(() => {
    if (!isListening) return;
    setInputText(transcript.trim());
  }, [transcript, isListening]);

  // ìŒì„± ì¸ì‹ ì—ëŸ¬ ì²˜ë¦¬
  useEffect(() => {
    const handleError = (event: any) => {
      console.error('Speech recognition error:', event);
      setIsListening(false);
      
      // ì‚¬ìš©ìì—ê²Œ ë” ìì„¸í•œ ì—ëŸ¬ ë©”ì‹œì§€ í‘œì‹œ
      let errorMessage = "Sorry, there was a problem with the speech recognition. ";
      if (event.error === 'no-speech') {
        errorMessage += "No speech was detected. Please try again.";
      } else if (event.error === 'audio-capture') {
        errorMessage += "Please check your microphone.";
      } else if (event.error === 'not-allowed') {
        errorMessage += "Please allow microphone access.";
      } else {
        errorMessage += "Please try again.";
      }
      
      setMessages(prev => [...prev, {
        text: errorMessage,
        sender: 'system'
      }]);
    };

    if (browserSupportsSpeechRecognition) {
      window.addEventListener('error', handleError);
    }

    return () => {
      if (browserSupportsSpeechRecognition) {
        window.removeEventListener('error', handleError);
      }
    };
  }, [browserSupportsSpeechRecognition]);

  const stopAIVoice = () => {
    if (currentUtterance.current) {
      window.speechSynthesis.cancel();
      currentUtterance.current = null;
    }
  };

  // PWA ì—…ë°ì´íŠ¸ í•¸ë“¤ëŸ¬
  const handleUpdate = () => {
    if ('serviceWorker' in navigator) {
      navigator.serviceWorker.ready.then(registration => {
        if (registration.waiting) {
          registration.waiting.postMessage({ type: 'SKIP_WAITING' });
        }
      });
    }
    setIsUpdateAvailable(false);
  };

  // ì´ˆê¸° ë©”ì‹œì§€ ì„¤ì •
  useEffect(() => {
    if (messages.length === 0) {
      setMessages([VERSION_INFO, WELCOME_MESSAGE]);
      // ì›°ì»´ ë©”ì‹œì§€ ìŒì„± ì¶œë ¥
      setTimeout(() => {
        speakResponse(WELCOME_MESSAGE.text);
      }, 1000);
    }
  }, []);

  const handleKeyPress = (e: React.KeyboardEvent) => {
    if (e.key === 'Enter' && !e.shiftKey) {
      e.preventDefault();
      handleSend();
    }
  };

  // ì„¤ì¹˜ í”„ë¡¬í”„íŠ¸ í‘œì‹œ í•¨ìˆ˜
  const handleInstallClick = () => {
    if (deferredPrompt) {
      deferredPrompt.prompt();
      deferredPrompt.userChoice.then((choiceResult: { outcome: string }) => {
        if (choiceResult.outcome === 'accepted') {
          console.log('User accepted the install prompt');
        }
        setDeferredPrompt(null);
        setShowInstallPrompt(false);
      });
    }
  };

  if (!browserSupportsSpeechRecognition) {
    return <div>Browser doesn't support speech recognition.</div>;
  }

  if (!isMicrophoneAvailable) {
    return <div>Please allow microphone access to use voice input.</div>;
  }

  return (
    <div className="chat-interface">
      {showInstallPrompt && (
        <div className="install-prompt">
          <div className="install-prompt-content">
            <p>ğŸ“± ì•±ì„ ì„¤ì¹˜í•˜ë©´ ë” í¸ë¦¬í•˜ê²Œ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!</p>
            <button onClick={handleInstallClick}>ì„¤ì¹˜í•˜ê¸°</button>
            <button onClick={() => setShowInstallPrompt(false)}>ë‚˜ì¤‘ì—</button>
          </div>
        </div>
      )}
      {isUpdateAvailable && (
        <div className="install-prompt">
          <div className="install-prompt-content">
            <p>ğŸ”„ ìƒˆë¡œìš´ ë²„ì „ì´ ìˆìŠµë‹ˆë‹¤. ì—…ë°ì´íŠ¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ?</p>
            <button onClick={handleUpdate}>ì—…ë°ì´íŠ¸</button>
            <button onClick={() => setIsUpdateAvailable(false)}>ë‚˜ì¤‘ì—</button>
          </div>
        </div>
      )}
      <div className="title-container">
        <h1 className="chat-title">
          <div className="title-main">
            <span>ğŸ’¬</span>
            <span>NoPlan, JustTalk</span>
            <span className="version">v1.0.11</span>
          </div>
        </h1>
        <div className="chat-subtitle">ë§‰ë¬´ê°€ë‚´ ì˜ì–´íšŒí™”</div>
      </div>
      
      <div className="chat-messages">
        {messages.map((message, index) => (
          <div key={index} className={`message-container ${message.sender}-container`}>
            <div className={`message ${message.sender}-message`}>
              {message.text}
              {message.sender === 'assistant' && !isListening && (
                <button 
                  className="replay-button"
                  onClick={() => speakResponse(message.text)}
                  aria-label="Replay message"
                  disabled={isListening}
                >
                  ğŸ”Š
                </button>
              )}
            </div>
          </div>
        ))}
        {loading && !isListening && (
          <div className="message-container assistant-container">
            <div className="message assistant-message">
              <div className="typing-indicator">
                <span></span>
                <span></span>
                <span></span>
              </div>
            </div>
          </div>
        )}
        <div ref={messagesEndRef} />
      </div>

      <div className="chat-controls">
        <div className="chat-input-container">
          <input
            type="text"
            value={inputText}
            onChange={(e) => setInputText(e.target.value)}
            onKeyPress={handleKeyPress}
            placeholder={isListening ? 'Listening...' : 'Type your message...'}
            className="chat-input"
            disabled={loading || isListening}
          />
          <div className="button-container">
            <button
              onClick={handleMicClick}
              className={`mic-button ${isListening ? 'stop' : 'start'}`}
              disabled={loading && !isListening}
            >
              {isListening ? 'ğŸ”´ Stop' : 'ğŸ¤ Start'}
            </button>
            <button
              onClick={handleSend}
              disabled={loading || !inputText.trim() || isListening}
              className="send-button"
            >
              {loading ? 'Sending...' : 'Send'}
            </button>
          </div>
        </div>
        {isListening && (
          <div className="listening-indicator">
            Listening... Speak in English
          </div>
        )}
      </div>
    </div>
  );
}

export default App;
